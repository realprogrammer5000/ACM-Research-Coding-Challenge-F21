{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with NLTK and VADER\n",
    "# TODO: decide on 'I' vs 'we'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load the input text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 50 characters of input file: \"Stop blushing. I'm not needling, really I'm not. \n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# read and close the file in one line\n",
    "# see https://stackoverflow.com/a/49564464\n",
    "text_file = Path(\"input.txt\").read_text()\n",
    "\n",
    "# ensure that we've read the correct file\n",
    "print(\"Read 50 characters of input file: \" + text_file[0:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Step 2: Split into two paragraphs and reformat text\n",
    "The input text file seems to contain two paragraphs that I believe should be analyzed separately.\n",
    "These paragraphs are seperated by double newlines, so I split them here.\n",
    "The input text file also contains newlines to avoid any one line from being too long, so I then remove all the newlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 2 paragraphs '\"Stop blushing. I'm ...' 'I think you may like...'\n"
     ]
    }
   ],
   "source": [
    "# split into paragraphs and remove newlines inside paragraphs\n",
    "# we also need to remove double-spaces because some lines end with a space\n",
    "paragraphs = text_file.split(\"\\n\\n\")\n",
    "paragraphs = [paragraph.replace(\"\\n\", \" \").replace(\"  \", \" \") for paragraph in paragraphs]\n",
    "print(\"Extracted {} paragraphs: '{}...' '{}...'\".format(len(paragraphs), paragraphs[0][0:20], paragraphs[1][0:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 3: Process paragraphs into words\n",
    "At this stage, we have two paragraphs to analyze. We would like to first split each paragraph into a list of words:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 3.1: Import and install\n",
    "We use [NLTK](https://www.nltk.org/) to tokenize our words here, leveraging the existing\n",
    "[Punkt](https://www.nltk.org/_modules/nltk/tokenize/punkt.html) tokenizer to do the heavy lifting\n",
    "for us. First we must import NLTK and install the `punkt` resource. We also download the `stopwords`\n",
    "resource for later."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized test sentence: ['Test', 'sentence', 'here', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/max/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/max/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "print(\"Tokenized test sentence: {}\".format(word_tokenize(\"Test sentence here.\")))\n",
    "\n",
    "# we will use this later\n",
    "nltk.download(\"stopwords\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 3.2: Tokenize words\n",
    "Here we actually call the word_tokenize() method."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized 2 paragraphs: ['``', 'Stop', 'blushing', '.'] ['I', 'think', 'you', 'may']\n"
     ]
    }
   ],
   "source": [
    "tokenized = [word_tokenize(paragraph) for paragraph in paragraphs]\n",
    "print(\"Tokenized paragraphs: {} {}\".format(tokenized[0][0:4], tokenized[1][0:4]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 3.3: Remove non-words\n",
    "As we saw from the previous output, the tokenized list includes \"words\" such as `'.'` that we don't want to\n",
    "analyze. We then remove them:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed non-words from paragraphs: ['stop', 'blushing', 'i', 'not', 'needling'] ['i', 'think', 'you', 'may', 'like']\n"
     ]
    }
   ],
   "source": [
    "# using a nested list comprehension because we need to iterate over a nested list\n",
    "# using str.isalpha(), assuming that we only want to keep words that have an alpha score\n",
    "# we also lowercase every word, as the case of words does not contribute meaningfully\n",
    "# to the sentiment of the text\n",
    "tokenized_words = [[word.lower() for word in paragraph if word.isalpha()] for paragraph in tokenized]\n",
    "\n",
    "print(\"Removed non-words from paragraphs: {} {}\".format(tokenized_words[0][0:5], tokenized_words[1][0:5]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that at this point we have lost sentence structure. An improved analysis might require keeping this structure."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 3.4: Remove \"stopwords\"\n",
    "As we saw from the previous output, there are many words such as \"I\" and \"you\" that do not contribute\n",
    "much to the sentiment of the text. These words occur frequently in English and are known as stopwords."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded stopwords set: ['i', 'me', 'my', 'myself', 'we']\n",
      "Removed stopwords from paragraphs: ['stop', 'blushing', 'needling', 'really', 'know'] ['think', 'may', 'like', 'know', 'something']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_set = stopwords.words(\"english\")\n",
    "# print a few of the stopwords so we can see them\n",
    "print(\"Loaded stopwords set: {}\".format(stopwords_set[0:5]))\n",
    "# note that all of these words are lowercase, just like the words in tokenized_words\n",
    "\n",
    "# remove stop-words from dataset\n",
    "tokenized_words_filtered = [[word for word in paragraph if not (word in stopwords_set)] for paragraph in tokenized_words]\n",
    "\n",
    "# print a few of the filtered words\n",
    "print(\"Removed stopwords from paragraphs: {} {}\".format(tokenized_words_filtered[0][0:5], tokenized_words_filtered[1][0:5]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "At this point we have properly formatted our input file into the list `tokenized_words_filtered`, containing two lists\n",
    "of the words in the input text files, excluding punctuation and words that do not contribute meaningfully\n",
    "to the sentiment of the text. We have also transformed every word into lower-case to make it easier to process."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}